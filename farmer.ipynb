{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijDG3IDqkjKx"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Let us build a Small Language Model (SLM) from scratch. We will try to keep the parameter size to 50-60 million.\n",
        "\n",
        "Our goal is to generate creative and coherent text based on the input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPl23BNsRqfm"
      },
      "source": [
        "## Step 1: Import the Agriculture Q&A Dataset\n",
        "\n",
        "We will use the KisanVaani agriculture-qa-english-only dataset from HuggingFace. This dataset contains 22,615 question-answer pairs about farming practices, crop management, soil health, pest control, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSeXbOztRtKN",
        "outputId": "9402b3b2-ce25-485f-b34a-58cf1ae0e9cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkpPWqR8-tFO",
        "outputId": "657426a7-d2c2-4b76-e968-9330c2cb8b91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "pip install -U datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r34TTsnR3GI",
        "outputId": "cff79286-9fab-440a-f3ad-9197b0801fa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully!\n",
            "Total samples: 22615\n",
            "Columns: ['question', 'answers']\n",
            "\n",
            "Training samples: 20353\n",
            "Validation samples: 2262\n",
            "\n",
            "--- Sample Q&A pairs ---\n",
            "\n",
            "Example 1:\n",
            "Q: Can I use chemicals to kill weeds in a cassava garden?\n",
            "A: Yes, there are some chemicals that are selective and can effectively kill weeds in a cassava garden ...\n",
            "\n",
            "Example 2:\n",
            "Q: what are combine harvesters?\n",
            "A: machines, which are loaded with technology, are very efficient and combine all three jobs of cutting...\n",
            "\n",
            "Example 3:\n",
            "Q: how do we call the cover crops that increases soil fertility.\n",
            "A: green manure...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the agriculture Q&A dataset directly via pandas (to avoid numpy version conflicts)\n",
        "url = \"https://huggingface.co/datasets/KisanVaani/agriculture-qa-english-only/resolve/main/data/train-00000-of-00001.parquet\"\n",
        "df = pd.read_parquet(url)\n",
        "\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "# Split into train and validation (90-10 split)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"\\nTraining samples: {len(train_df)}\")\n",
        "print(f\"Validation samples: {len(val_df)}\")\n",
        "\n",
        "# Show sample Q&A\n",
        "print(f\"\\n--- Sample Q&A pairs ---\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Q: {train_df.iloc[i]['question']}\")\n",
        "    print(f\"A: {train_df.iloc[i]['answers'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vccyr4qKR6OH"
      },
      "source": [
        "## Step 2: Tokenize the Agriculture Q&A Dataset\n",
        "\n",
        "In this step, we will:\n",
        "\n",
        "(1) Format the Q&A pairs into instruction format: \"Question: [q]\\nAnswer: [a]\"\n",
        "\n",
        "(2) Tokenize the dataset into tokenIDs.\n",
        "\n",
        "(3) Create \"train.bin\" and \"validation.bin\" files to store the tokenIDs.\n",
        "\n",
        "(4) Store tokenIDs on disk for efficient computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFkgAjyMR8fa",
        "outputId": "65548562-4db1-4cc7-975d-904a95df97d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary files already exist. Skipping tokenization.\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken scikit-learn -q\n",
        "import tiktoken\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def format_qa_pair(row):\n",
        "    \"\"\"Format question-answer pair into instruction format\"\"\"\n",
        "    question = row['question']\n",
        "    answer = row['answers']\n",
        "    # Instruction format for Q&A\n",
        "    text = f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
        "    return text\n",
        "\n",
        "def tokenize_and_save(df, filename):\n",
        "    \"\"\"Tokenize dataframe and save to binary file\"\"\"\n",
        "    print(f\"\\nTokenizing {filename}...\")\n",
        "\n",
        "    # Format all Q&A pairs\n",
        "    texts = [format_qa_pair(row) for _, row in df.iterrows()]\n",
        "\n",
        "    # Tokenize all texts\n",
        "    all_ids = []\n",
        "    for text in tqdm(texts, desc=f\"Processing {filename}\"):\n",
        "        ids = enc.encode_ordinary(text)\n",
        "        all_ids.extend(ids)\n",
        "\n",
        "    # Convert to numpy array and save\n",
        "    arr_len = len(all_ids)\n",
        "    dtype = np.uint16  # GPT-2 vocab size is 50257 < 2^16\n",
        "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
        "    arr[:] = all_ids\n",
        "    arr.flush()\n",
        "\n",
        "    print(f\"✓ Saved {arr_len:,} tokens to {filename}\")\n",
        "    return arr_len\n",
        "\n",
        "# Only create if files don't exist\n",
        "if not os.path.exists(\"train.bin\"):\n",
        "    train_tokens = tokenize_and_save(train_df, \"train.bin\")\n",
        "    val_tokens = tokenize_and_save(val_df, \"validation.bin\")\n",
        "\n",
        "    print(f\"\\n✓ Tokenization complete!\")\n",
        "    print(f\"  Training tokens: {train_tokens:,}\")\n",
        "    print(f\"  Validation tokens: {val_tokens:,}\")\n",
        "else:\n",
        "    print(\"Binary files already exist. Skipping tokenization.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_qRtn_WSbV4"
      },
      "source": [
        "## Step 3: Create Input-Output batches for the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gak79CZESkjN"
      },
      "outputs": [],
      "source": [
        "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/train.py with slight modifications\n",
        "#block size = context window\n",
        "def get_batch(split):\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA1SVp1hkjKy"
      },
      "source": [
        "## Step 4: Define the SLM Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "9friaxWABOA-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from contextlib import nullcontext\n",
        "import os\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "    def forward(self, x):\n",
        "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        # TorchScript needs the attribute to always exist and be a Tensor\n",
        "        self.bias: torch.Tensor = torch.tril(\n",
        "            torch.ones(config.block_size, config.block_size)\n",
        "        ).view(1, 1, config.block_size, config.block_size)\n",
        "\n",
        "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # [B, H, T, Dh]\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        if self.flash:\n",
        "            y = F.scaled_dot_product_attention(\n",
        "                q, k, v, attn_mask=None,\n",
        "                dropout_p=self.attn_dropout.p if self.training else 0.0,\n",
        "                is_causal=True\n",
        "            )\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            # slice the pre-registered mask; TorchScript-friendly\n",
        "            mask = self.bias[:, :, :T, :T]  # [1,1,T,T]\n",
        "            att = att.masked_fill(mask == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int\n",
        "    vocab_size: int\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_embd: int\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # ❌ don't keep the dataclass:\n",
        "        # self.config = config\n",
        "\n",
        "        # ✅ store primitive copies (TorchScript friendly)\n",
        "        self.block_size: int = int(config.block_size)\n",
        "        self.vocab_size: int = int(config.vocab_size)\n",
        "        self.n_layer: int = int(config.n_layer)\n",
        "        self.n_head: int = int(config.n_head)\n",
        "        self.n_embd: int = int(config.n_embd)\n",
        "        self.use_bias: bool = bool(config.bias)\n",
        "        self.drop_p: float = float(config.dropout)\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(self.vocab_size, self.n_embd),\n",
        "            wpe=nn.Embedding(self.block_size, self.n_embd),\n",
        "            drop=nn.Dropout(self.drop_p),\n",
        "            h=nn.ModuleList([Block(config) for _ in range(self.n_layer)]),\n",
        "            ln_f=LayerNorm(self.n_embd, self.use_bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(self.n_embd, self.vocab_size, bias=False)\n",
        "        # weight tying ok for TorchScript\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.n_layer))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "\n",
        "\n",
        "        if t > self.block_size:\n",
        "            idx = idx[:, -self.block_size:]\n",
        "            t = idx.size(1)\n",
        "\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(\n",
        "                logits.reshape(-1, logits.size(-1)),\n",
        "                targets.reshape(-1),\n",
        "                ignore_index=-1\n",
        "            )\n",
        "            return logits, loss\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])  # [B,1,V]\n",
        "            return logits, None\n",
        "\n",
        "    @torch.jit.ignore   # keep generate out of TorchScript\n",
        "    def generate(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Generate tokens given a conditioning sequence.\n",
        "        idx: Tensor of shape (B, T)\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-20T15:36:54.88378Z",
          "iopub.status.busy": "2024-06-20T15:36:54.883342Z",
          "iopub.status.idle": "2024-06-20T15:37:07.278493Z",
          "shell.execute_reply": "2024-06-20T15:37:07.277342Z",
          "shell.execute_reply.started": "2024-06-20T15:36:54.883753Z"
        },
        "id": "uRm6WlvfkjKz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "config = GPTConfig(\n",
        "    vocab_size=50257,     # use the tokenizer's vocab size\n",
        "    block_size=128,       # or whatever context size you're training with\n",
        "    n_layer=6,\n",
        "    n_head=6,\n",
        "    n_embd=384,\n",
        "    dropout=0.1,\n",
        "    bias=True\n",
        ")\n",
        "\n",
        "model = GPT(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_a8Rd-0S_WC"
      },
      "source": [
        "## Step 5: Define the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "La2Aun_nTBzk"
      },
      "outputs": [],
      "source": [
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            for k in range(eval_iters):\n",
        "                X, Y = get_batch(split)\n",
        "                with ctx:\n",
        "                    logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "            out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvqWPUstTRXO"
      },
      "source": [
        "## Step 6: Define SLM Training Configuration Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QQyayhnkjK5",
        "outputId": "e7d64b10-b7ab-43be-daff-f06ecb28015a",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e7d1bd37570>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training Config\n",
        "import torch\n",
        "from contextlib import nullcontext\n",
        "\n",
        "learning_rate = 1e-4 #more stable training, earlier 1e-4\n",
        "max_iters = 20000 #increase from 25000\n",
        "warmup_steps = 1000 #smoother initial train, earlier 100\n",
        "min_lr = 5e-4 #lower rate, earlier 5e-4\n",
        "eval_iters = 500 # increased from 100\n",
        "batch_size = 32 # changed from 16, better gradient estimate\n",
        "block_size = 128 #changed from 64, capture longer range dependencies\n",
        "\n",
        "gradient_accumulation_steps = 32 # reduced from 50\n",
        "\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "\n",
        "# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n",
        "#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "torch.set_default_device(device)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmWj6YcKTW_z"
      },
      "source": [
        "## Step 7: Define SLM Training Configuration Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JMkO3FlNkjK6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
        "\n",
        "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n",
        "\n",
        "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
        "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
        "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
        "\n",
        "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz8fPSKNTY3W"
      },
      "source": [
        "## Step 8: Pre-train the SLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7fc09e354ce24e9eaadc6378ce6953bc",
            "3128f40e72df4405b0c17046734d3d94",
            "181545ca9a104f2381d2456c888d9a9c",
            "0f015e2dafb14988b8bd74033e2a67f9",
            "65ad3a86768b4ff1886bbe671dde8628",
            "634e38a972154967a3eba88b6fb68af9",
            "47edcf5379fe48f996132064a496a663",
            "d12123b73fd74e6688c4bc39477373a4",
            "a04cc62546dd4c8694b657c576813987",
            "41530173454b4cf3870d476ef7df20bd",
            "5fb6f8f023b24e129000d9a7a4b7ffe7"
          ]
        },
        "id": "t0l-YhockjK6",
        "outputId": "6aafd2a3-1168-4b53-c4a9-e53fbaa61580",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fc09e354ce24e9eaadc6378ce6953bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 500: train loss 9.2750, val loss 9.2509\n",
            "The current learning rate: 0.00007\n",
            "Epoch 1000: train loss 8.3805, val loss 8.3592\n",
            "The current learning rate: 0.00010\n",
            "Epoch 1500: train loss 7.5455, val loss 7.5293\n",
            "The current learning rate: 0.00010\n",
            "Epoch 2000: train loss 6.7835, val loss 6.7692\n",
            "The current learning rate: 0.00010\n",
            "Epoch 2500: train loss 6.1444, val loss 6.1321\n",
            "The current learning rate: 0.00011\n",
            "Epoch 3000: train loss 5.6577, val loss 5.6523\n",
            "The current learning rate: 0.00011\n",
            "Epoch 3500: train loss 5.2426, val loss 5.2334\n",
            "The current learning rate: 0.00012\n",
            "Epoch 4000: train loss 4.9083, val loss 4.9095\n",
            "The current learning rate: 0.00012\n",
            "Epoch 4500: train loss 4.6206, val loss 4.6274\n",
            "The current learning rate: 0.00013\n",
            "Epoch 5000: train loss 4.3413, val loss 4.3605\n",
            "The current learning rate: 0.00014\n",
            "Epoch 5500: train loss 4.1245, val loss 4.1380\n",
            "The current learning rate: 0.00015\n",
            "Epoch 6000: train loss 3.8987, val loss 3.9159\n",
            "The current learning rate: 0.00016\n",
            "Epoch 6500: train loss 3.6836, val loss 3.7105\n",
            "The current learning rate: 0.00018\n",
            "Epoch 7000: train loss 3.4847, val loss 3.5142\n",
            "The current learning rate: 0.00019\n",
            "Epoch 7500: train loss 3.2608, val loss 3.2934\n",
            "The current learning rate: 0.00020\n",
            "Epoch 8000: train loss 3.0498, val loss 3.0997\n",
            "The current learning rate: 0.00022\n",
            "Epoch 8500: train loss 2.8531, val loss 2.9103\n",
            "The current learning rate: 0.00024\n",
            "Epoch 9000: train loss 2.6301, val loss 2.6973\n",
            "The current learning rate: 0.00025\n",
            "Epoch 9500: train loss 2.4278, val loss 2.4883\n",
            "The current learning rate: 0.00027\n",
            "Epoch 10000: train loss 2.2084, val loss 2.2713\n",
            "The current learning rate: 0.00028\n",
            "Epoch 10500: train loss 1.9930, val loss 2.0786\n",
            "The current learning rate: 0.00030\n",
            "Epoch 11000: train loss 1.7783, val loss 1.8718\n",
            "The current learning rate: 0.00032\n",
            "Epoch 11500: train loss 1.5636, val loss 1.6651\n",
            "The current learning rate: 0.00033\n",
            "Epoch 12000: train loss 1.3673, val loss 1.4627\n",
            "The current learning rate: 0.00035\n",
            "Epoch 12500: train loss 1.1844, val loss 1.2892\n",
            "The current learning rate: 0.00036\n",
            "Epoch 13000: train loss 1.0226, val loss 1.1286\n",
            "The current learning rate: 0.00038\n",
            "Epoch 13500: train loss 0.8885, val loss 0.9805\n",
            "The current learning rate: 0.00040\n",
            "Epoch 14000: train loss 0.7575, val loss 0.8440\n",
            "The current learning rate: 0.00041\n",
            "Epoch 14500: train loss 0.6591, val loss 0.7375\n",
            "The current learning rate: 0.00042\n",
            "Epoch 15000: train loss 0.5824, val loss 0.6736\n",
            "The current learning rate: 0.00044\n",
            "Epoch 15500: train loss 0.5132, val loss 0.6059\n",
            "The current learning rate: 0.00045\n",
            "Epoch 16000: train loss 0.4580, val loss 0.5479\n",
            "The current learning rate: 0.00046\n",
            "Epoch 16500: train loss 0.4332, val loss 0.5196\n",
            "The current learning rate: 0.00047\n",
            "Epoch 17000: train loss 0.3966, val loss 0.4936\n",
            "The current learning rate: 0.00048\n",
            "Epoch 17500: train loss 0.3687, val loss 0.4749\n",
            "The current learning rate: 0.00048\n",
            "Epoch 18000: train loss 0.3461, val loss 0.4480\n",
            "The current learning rate: 0.00049\n",
            "Epoch 18500: train loss 0.3286, val loss 0.4352\n",
            "The current learning rate: 0.00049\n",
            "Epoch 19000: train loss 0.3106, val loss 0.4210\n",
            "The current learning rate: 0.00050\n",
            "Epoch 19500: train loss 0.2984, val loss 0.4055\n",
            "The current learning rate: 0.00050\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf')\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "train_loss_list, validation_loss_list = [], []\n",
        "\n",
        "# Ensure model is on the correct device\n",
        "model = model.to(device)\n",
        "\n",
        "# In your training loop\n",
        "for epoch in tqdm(range(max_iters)):\n",
        "    if epoch % eval_iters == 0 and epoch != 0:\n",
        "        # Ensure estimate_loss uses the correct device\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
        "        train_loss_list += [losses['train']]\n",
        "        validation_loss_list += [losses['val']]\n",
        "\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "    # Ensure X and y are on the correct device\n",
        "    X, y = get_batch(\"train\")\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    with ctx:\n",
        "        logits, loss = model(X, y)\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdzhSo_7TcgI"
      },
      "source": [
        "\n",
        "## Step 9: Plot the SLM Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "KSWpvAnakjK6",
        "outputId": "939de6ee-e914-404e-f865-80b226622e61",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
        "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
        "\n",
        "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
        "plt.plot(validation_loss_list_converted, 'r', label='validation_loss')\n",
        "plt.xlabel(\"Steps - Every 100 epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2ocyyQwkjK6"
      },
      "source": [
        "## Step 10: Run SLM Inference on our trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-20T15:45:44.322237Z",
          "iopub.status.busy": "2024-06-20T15:45:44.321316Z",
          "iopub.status.idle": "2024-06-20T15:45:46.887084Z",
          "shell.execute_reply": "2024-06-20T15:45:46.886126Z",
          "shell.execute_reply.started": "2024-06-20T15:45:44.322203Z"
        },
        "id": "06NrdWKdkjK7",
        "outputId": "51c0bdc5-a585-4811-a5b1-d94995722e0d",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Load the model\n",
        "model = GPT(config)  # re-create the model with same config\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-20T15:44:36.64937Z",
          "iopub.status.busy": "2024-06-20T15:44:36.64896Z",
          "iopub.status.idle": "2024-06-20T15:45:14.425576Z",
          "shell.execute_reply": "2024-06-20T15:45:14.424712Z",
          "shell.execute_reply.started": "2024-06-20T15:44:36.649341Z"
        },
        "id": "K8PgWXb-kjK7",
        "outputId": "ae457f9e-725f-437a-e0ce-5762d1ef3dfd",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: What is the best time to plant rice?\n",
            "\n",
            "Full Response:\n",
            "Question: What is the best time to plant rice?\n",
            "Answer: The timing and method of fertilizer application, such as surface application, broadcasting, or placement, can influence nutrient availability and uptake by crops. For example, applying nitrogen fertilizer at planting may promote early crop growth and development, while side-dressing or top-dressing may promote later growth and fruiting.\n",
            "\n",
            "Question: what is depth of the soil for planting maize.\n",
            "Answer: make an estimate of about 5-7 cm deep, and you can make the holes deeper for dry planting and sandy\n"
          ]
        }
      ],
      "source": [
        "# Test with an agriculture question\n",
        "question = \"What is the best time to plant rice?\"\n",
        "prompt = f\"Question: {question}\\nAnswer:\"\n",
        "\n",
        "context = torch.tensor(enc.encode_ordinary(prompt)).unsqueeze(dim=0)\n",
        "y = model.generate(context, 100, temperature=0.8, top_k=40)\n",
        "response = enc.decode(y.squeeze().tolist())\n",
        "\n",
        "# Extract just the answer part\n",
        "print(f\"Q: {question}\")\n",
        "print(f\"\\nFull Response:\\n{response}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZsL_mONRZAO",
        "outputId": "be1181ae-9c1a-4ce0-f198-a639b9baf441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: How do I control pests in my crops?\n",
            "\n",
            "Full Response:\n",
            "Question: How do I control pests in my crops?\n",
            "Answer: Farmers are know nitrogen-absorbing kigezi farming used to protect crops in your citrus cultivation. It's effective against sustainable farming organization, such as abscisic acid and trell, generally known as remote sensing, high humidity levels, and early maintanance.\n",
            "\n",
            "Question: Does the United States import more agricultural products than we export?\n",
            "\n",
            "Answer: Agriculture has a positive trade balance, which means we send out (export) more than we bring in (import)\n",
            "\n",
            "Question\n",
            "\n",
            "============================================================\n",
            "\n",
            "Q: What are the signs of nutrient deficiency in soil?\n",
            "\n",
            "Full Response:\n",
            "Question: What are the signs of nutrient deficiency in soil?\n",
            "Answer: stunted growth, distorted twigs, and reduced yield.\n",
            "\n",
            "Question: What are the symptoms of Pythium root rot?\n",
            "Answer: The pathogens responsible for Pythium root rot include Pythium root rot, soil-borne off, and Rhizoctonia\n",
            "\n",
            "Question: which nutrient is essential for the production of chlorophyll.\n",
            "Answer: Nitrogen \n",
            "\n",
            "Question: how do we call the cover crops that increases soil fertility.\n",
            "Answer: green manure\n",
            "\n",
            "Question: what is the maturity\n"
          ]
        }
      ],
      "source": [
        "# Test with another agriculture question\n",
        "question = \"How do I control pests in my crops?\"\n",
        "prompt = f\"Question: {question}\\nAnswer:\"\n",
        "\n",
        "context = torch.tensor(enc.encode_ordinary(prompt)).unsqueeze(dim=0)\n",
        "y = model.generate(context, 100, temperature=0.8, top_k=40)\n",
        "response = enc.decode(y.squeeze().tolist())\n",
        "\n",
        "print(f\"Q: {question}\")\n",
        "print(f\"\\nFull Response:\\n{response}\")\n",
        "\n",
        "# Try more questions\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "question = \"What are the signs of nutrient deficiency in soil?\"\n",
        "prompt = f\"Question: {question}\\nAnswer:\"\n",
        "context = torch.tensor(enc.encode_ordinary(prompt)).unsqueeze(dim=0)\n",
        "y = model.generate(context, 100, temperature=0.8, top_k=40)\n",
        "response = enc.decode(y.squeeze().tolist())\n",
        "\n",
        "print(f\"Q: {question}\")\n",
        "print(f\"\\nFull Response:\\n{response}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_2WjvUszhIe"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL54CFCgq8cf"
      },
      "outputs": [],
      "source": [
        "# Helper function to ask agriculture questions\n",
        "def ask_farming_question(question, max_tokens=150, temperature=0.7, top_k=40):\n",
        "    \"\"\"\n",
        "    Ask the model an agriculture-related question\n",
        "\n",
        "    Args:\n",
        "        question: The farmer's question\n",
        "        max_tokens: Maximum tokens to generate\n",
        "        temperature: Sampling temperature (lower = more focused)\n",
        "        top_k: Top-k sampling parameter\n",
        "    \"\"\"\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "    context = torch.tensor(enc.encode_ordinary(prompt)).unsqueeze(dim=0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y = model.generate(context, max_tokens, temperature=temperature, top_k=top_k)\n",
        "\n",
        "    response = enc.decode(y.squeeze().tolist())\n",
        "\n",
        "    # Extract just the answer (remove the question part)\n",
        "    if \"Answer:\" in response:\n",
        "        answer = response.split(\"Answer:\", 1)[1].split(\"Question:\")[0].strip()\n",
        "    else:\n",
        "        answer = response\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Test the helper function with various farming questions\n",
        "print(\"🌾 Agriculture Q&A Model - Interactive Testing 🌾\\n\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "questions = [\n",
        "    \"What is crop rotation?\",\n",
        "    \"How can I improve soil fertility?\",\n",
        "    \"What causes yellowing of leaves in plants?\",\n",
        "    \"When should I water my crops?\",\n",
        "    \"How do I prevent fungal diseases in crops?\"\n",
        "]\n",
        "\n",
        "for i, q in enumerate(questions, 1):\n",
        "    print(f\"\\n[Q{i}] {q}\")\n",
        "    answer = ask_farming_question(q)\n",
        "    print(f\"[A{i}] {answer}\")\n",
        "    print(\"-\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Ql2No-lPdBSk",
        "outputId": "b6b2b175-8c2d-4732-9a03-abd38b2aab98"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_1c1467ae-b5cc-4ed2-a84a-d44196ed8b5c\", \"best_model_params.pt\", 120011315)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"best_model_params.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5nEtDLiodDP-",
        "outputId": "aa27ca29-c02c-468b-8194-193bc030427d"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_c8d35b1c-19fa-4060-a0b6-0953a2a87ea0\", \"best_model_params.pt\", 120011315)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_a4f52d9c-75a0-422b-8320-1986ae3280d4\", \"config.json\", 112)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_ee1b007b-7bc0-4d6a-adde-e212249e8000\", \"meta.json\", 49)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# create config/meta and download\n",
        "import json, torch\n",
        "json.dump(vars(config), open(\"config.json\",\"w\"))\n",
        "json.dump({\"tokenizer\":\"tiktoken:gpt2\",\"block_size\": config.block_size}, open(\"meta.json\",\"w\"))\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"best_model_params.pt\")\n",
        "files.download(\"config.json\")\n",
        "files.download(\"meta.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "p_1lKWONhKke",
        "outputId": "165e0984-0170-47a5-850c-b160b51e50fd"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "\nModule 'CausalSelfAttention' has no attribute 'bias' :\n  File \"/tmp/ipython-input-4040496400.py\", line 45\n        else:\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n                                  ~~~~~~~~~ <--- HERE\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-852767510.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best_model_params.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mm_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscripted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mscripted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"farmer_model_torchscript.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TOPLEVEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0m_TOPLEVEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m         ret = _script_impl(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_script_impl\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_prepare_scriptable_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m         return torch.jit._recursive.create_script_module(\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recursive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_methods_to_compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mAttributeTypeIsSupportedChecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_script_module_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \"\"\"\n\u001b[1;32m    652\u001b[0m             \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;31m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;31m# always reuse the provided stubs_fn to infer the methods to compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 scripted = create_script_module_impl(\n\u001b[0m\u001b[1;32m    603\u001b[0m                     \u001b[0morig_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_concrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \"\"\"\n\u001b[1;32m    652\u001b[0m             \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;31m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;31m# always reuse the provided stubs_fn to infer the methods to compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 scripted = create_script_module_impl(\n\u001b[0m\u001b[1;32m    603\u001b[0m                     \u001b[0morig_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_concrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \"\"\"\n\u001b[1;32m    652\u001b[0m             \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;31m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;31m# always reuse the provided stubs_fn to infer the methods to compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 scripted = create_script_module_impl(\n\u001b[0m\u001b[1;32m    603\u001b[0m                     \u001b[0morig_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_concrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \"\"\"\n\u001b[1;32m    652\u001b[0m             \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;31m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;31m# always reuse the provided stubs_fn to infer the methods to compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 scripted = create_script_module_impl(\n\u001b[0m\u001b[1;32m    603\u001b[0m                     \u001b[0morig_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_concrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconcrete_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconcrete_type_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethods_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         create_methods_and_properties_from_stubs(\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mconcrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_stubs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperty_stubs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0mproperty_rcbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolution_callback\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperty_stubs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m     concrete_type._create_methods_and_properties(\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0mproperty_defs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperty_rcbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_defs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_rcbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \nModule 'CausalSelfAttention' has no attribute 'bias' :\n  File \"/tmp/ipython-input-4040496400.py\", line 45\n        else:\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n                                  ~~~~~~~~~ <--- HERE\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n"
          ]
        }
      ],
      "source": [
        "# export quantized TorchScript now (optional, recommended)\n",
        "m = GPT(config); m.load_state_dict(torch.load(\"best_model_params.pt\", map_location=\"cpu\")); m.eval()\n",
        "m_q = torch.quantization.quantize_dynamic(m, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "scripted = torch.jit.script(m_q)\n",
        "scripted.save(\"farmer_model_torchscript.pt\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"farmer_model_torchscript.pt\")\n",
        "# download vocab/merges you added to the VM\n",
        "# files.download(\"vocab.json\"); files.download(\"merges.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "kcX33G6YhftN",
        "outputId": "cfdd125b-8ddf-414a-ce5f-2dcc46b7b7dd"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_17bf9228-c6b5-44ef-b585-2d2f53479466\", \"ckpt_full.pt\", 360040011)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# save a resumable checkpoint (optional)\n",
        "ckpt = {\n",
        "  \"model\": m.state_dict(),\n",
        "  \"optimizer\": optimizer.state_dict(),\n",
        "  \"scheduler\": scheduler.state_dict(),\n",
        "  \"scaler\": scaler.state_dict() if 'scaler' in globals() else None,\n",
        "  \"config\": vars(config),\n",
        "  \"step\": max_iters\n",
        "}\n",
        "torch.save(ckpt, \"ckpt_full.pt\")\n",
        "from google.colab import files; files.download(\"ckpt_full.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "eGVliMSsiUct",
        "outputId": "6c2e5e1e-7366-492b-9bfd-e3ab0180ef10"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "\nUnsupported operation: attempted to use multidimensional indexing on a non-tensor type:\n  File \"/tmp/ipython-input-1291681504.py\", line 47\n        else:\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n                                  ~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4288623839.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Sanity check scripting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mscripted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mscripted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"farmer_model_torchscript.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved farmer_model_torchscript.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TOPLEVEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0m_TOPLEVEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m         ret = _script_impl(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_script_impl\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_prepare_scriptable_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m         return torch.jit._recursive.create_script_module(\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recursive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_methods_to_compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mAttributeTypeIsSupportedChecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_script_module_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \"\"\"\n\u001b[1;32m    652\u001b[0m             \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;31m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;31m# always reuse the provided stubs_fn to infer the methods to compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 scripted = create_script_module_impl(\n\u001b[0m\u001b[1;32m    603\u001b[0m                     \u001b[0morig_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_concrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \"\"\"\n\u001b[1;32m    652\u001b[0m             \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;31m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;31m# always reuse the provided stubs_fn to infer the methods to compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 scripted = create_script_module_impl(\n\u001b[0m\u001b[1;32m    603\u001b[0m                     \u001b[0morig_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_concrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \"\"\"\n\u001b[1;32m    652\u001b[0m             \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;31m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;31m# always reuse the provided stubs_fn to infer the methods to compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 scripted = create_script_module_impl(\n\u001b[0m\u001b[1;32m    603\u001b[0m                     \u001b[0morig_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_concrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \"\"\"\n\u001b[1;32m    652\u001b[0m             \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;31m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;31m# always reuse the provided stubs_fn to infer the methods to compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 scripted = create_script_module_impl(\n\u001b[0m\u001b[1;32m    603\u001b[0m                     \u001b[0morig_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_concrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \"\"\"\n\u001b[1;32m    652\u001b[0m             \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;31m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;31m# always reuse the provided stubs_fn to infer the methods to compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 scripted = create_script_module_impl(\n\u001b[0m\u001b[1;32m    603\u001b[0m                     \u001b[0morig_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_concrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconcrete_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconcrete_type_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethods_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         create_methods_and_properties_from_stubs(\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mconcrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_stubs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperty_stubs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0mproperty_rcbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolution_callback\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperty_stubs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m     concrete_type._create_methods_and_properties(\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0mproperty_defs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperty_rcbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_defs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_rcbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \nUnsupported operation: attempted to use multidimensional indexing on a non-tensor type:\n  File \"/tmp/ipython-input-1291681504.py\", line 47\n        else:\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n                                  ~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n"
          ]
        }
      ],
      "source": [
        "import torch, json\n",
        "\n",
        "# Rebuild config + model and load your trained weights (no retrain)\n",
        "config = GPTConfig(**json.load(open(\"config.json\")))\n",
        "m = GPT(config)\n",
        "m.load_state_dict(torch.load(\"best_model_params.pt\", map_location=\"cpu\"))\n",
        "m.eval()\n",
        "\n",
        "# (Optional but recommended) Dynamic INT8 quantization for Linear layers\n",
        "m = torch.quantization.quantize_dynamic(m, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "# Export wrapper -> returns [B, V] logits for last position\n",
        "class ExportWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
        "        # idx: [B, T] int64\n",
        "        with torch.inference_mode():\n",
        "            logits, _ = self.model(idx)          # your model returns [B, 1, V] when targets=None\n",
        "            return logits[:, -1, :]              # -> [B, V]\n",
        "\n",
        "wrapper = ExportWrapper(m).eval()\n",
        "\n",
        "# Sanity check scripting\n",
        "scripted = torch.jit.script(wrapper)\n",
        "scripted.save(\"farmer_model_torchscript.pt\")\n",
        "print(\"Saved farmer_model_torchscript.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "4M19lb_ykFv9",
        "outputId": "755a3b63-f731-432b-b351-e9353a1e1c59"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "\nModule 'GPT' has no attribute 'config' (This attribute exists on the Python module, but we failed to convert Python type: '__main__.GPTConfig' to a TorchScript type. Only tensors and (possibly nested) tuples of tensors, lists, or dicts are supported as inputs or outputs of traced functions, but instead got value of type GPTConfig.. Its type was inferred; try adding a type annotation for the attribute.):\n  File \"/tmp/ipython-input-2179253721.py\", line 126\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size\n                    ~~~~~~~~~~~ <--- HERE\n        pos = torch.arange(0, t, dtype=torch.long, device=device)\n    \n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-103745858.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 4) Script & save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mscripted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mscripted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"farmer_model_torchscript.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ Exported farmer_model_torchscript.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TOPLEVEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0m_TOPLEVEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m         ret = _script_impl(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_script_impl\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_prepare_scriptable_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m         return torch.jit._recursive.create_script_module(\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recursive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_methods_to_compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mAttributeTypeIsSupportedChecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_script_module_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \"\"\"\n\u001b[1;32m    652\u001b[0m             \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;31m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;31m# always reuse the provided stubs_fn to infer the methods to compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 scripted = create_script_module_impl(\n\u001b[0m\u001b[1;32m    603\u001b[0m                     \u001b[0morig_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_concrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconcrete_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconcrete_type_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethods_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         create_methods_and_properties_from_stubs(\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mconcrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_stubs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperty_stubs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0mproperty_rcbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolution_callback\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperty_stubs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m     concrete_type._create_methods_and_properties(\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0mproperty_defs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperty_rcbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_defs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_rcbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \nModule 'GPT' has no attribute 'config' (This attribute exists on the Python module, but we failed to convert Python type: '__main__.GPTConfig' to a TorchScript type. Only tensors and (possibly nested) tuples of tensors, lists, or dicts are supported as inputs or outputs of traced functions, but instead got value of type GPTConfig.. Its type was inferred; try adding a type annotation for the attribute.):\n  File \"/tmp/ipython-input-2179253721.py\", line 126\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size\n                    ~~~~~~~~~~~ <--- HERE\n        pos = torch.arange(0, t, dtype=torch.long, device=device)\n    \n"
          ]
        }
      ],
      "source": [
        "import torch, json\n",
        "\n",
        "# 1) Rebuild & load weights\n",
        "config = GPTConfig(**json.load(open(\"config.json\")))\n",
        "m = GPT(config)\n",
        "m.load_state_dict(torch.load(\"best_model_params.pt\", map_location=\"cpu\"))\n",
        "m.eval()\n",
        "\n",
        "# 2) (Optional) Dynamic INT8 quantization for Linear layers\n",
        "m = torch.quantization.quantize_dynamic(m, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "# 3) Wrap to return last-token logits [B, V] (simpler on Android)\n",
        "class ExportWrapper(torch.nn.Module):\n",
        "    def __init__(self, model): super().__init__(); self.model = model\n",
        "    def forward(self, idx: torch.Tensor) -> torch.Tensor:  # idx: [B,T] int64\n",
        "        logits, _ = self.model(idx)            # your GPT forward when targets=None\n",
        "        return logits[:, -1, :]                # [B,V]\n",
        "\n",
        "wrapper = ExportWrapper(m).eval()\n",
        "\n",
        "# 4) Script & save\n",
        "scripted = torch.jit.script(wrapper)\n",
        "scripted.save(\"farmer_model_torchscript.pt\")\n",
        "print(\"✅ Exported farmer_model_torchscript.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Koi5oG9Rkw2u",
        "outputId": "91bc23e8-16ef-4c42-8990-a5ba8cd37f21"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Unsupported value kind: Tensor",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4244557883.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# try scripting first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mscripted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mscripted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"farmer_model_torchscript.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ TorchScript exported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TOPLEVEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0m_TOPLEVEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m         ret = _script_impl(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_script_impl\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_prepare_scriptable_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m         return torch.jit._recursive.create_script_module(\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recursive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_methods_to_compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mAttributeTypeIsSupportedChecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_script_module_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconcrete_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconcrete_type_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethods_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         create_methods_and_properties_from_stubs(\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mconcrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_stubs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperty_stubs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0mproperty_rcbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolution_callback\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperty_stubs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m     concrete_type._create_methods_and_properties(\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0mproperty_defs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperty_rcbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_defs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_rcbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsupported value kind: Tensor"
          ]
        }
      ],
      "source": [
        "import json, torch\n",
        "\n",
        "# rebuild from your saved config.json\n",
        "cfg = GPTConfig(**json.load(open(\"config.json\")))\n",
        "m = GPT(cfg)\n",
        "m.load_state_dict(torch.load(\"best_model_params.pt\", map_location=\"cpu\"))\n",
        "m.eval()\n",
        "\n",
        "# (optional) dynamic INT8 for size/speed\n",
        "m = torch.quantization.quantize_dynamic(m, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "# small wrapper so Android gets [B,V] logits for the last token\n",
        "class ExportWrapper(torch.nn.Module):\n",
        "    def __init__(self, model): super().__init__(); self.model = model\n",
        "    def forward(self, idx: torch.Tensor) -> torch.Tensor:  # [B,T] int64\n",
        "        logits, _ = self.model(idx)   # -> [B,1,V]\n",
        "        return logits[:, -1, :]       # -> [B,V]\n",
        "\n",
        "wrapper = ExportWrapper(m).eval()\n",
        "\n",
        "# try scripting first\n",
        "scripted = torch.jit.script(wrapper)\n",
        "scripted.save(\"farmer_model_torchscript.pt\")\n",
        "print(\"✅ TorchScript exported\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "OT64F5bKmAVd",
        "outputId": "810309ff-e160-4cc9-c160-3fe82ec634e7"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for GPT:\n\tMissing key(s) in state_dict: \"transformer.h.0.attn.bias\", \"transformer.h.1.attn.bias\", \"transformer.h.2.attn.bias\", \"transformer.h.3.attn.bias\", \"transformer.h.4.attn.bias\", \"transformer.h.5.attn.bias\". ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2744303710.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best_model_params.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2624\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2625\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2626\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GPT:\n\tMissing key(s) in state_dict: \"transformer.h.0.attn.bias\", \"transformer.h.1.attn.bias\", \"transformer.h.2.attn.bias\", \"transformer.h.3.attn.bias\", \"transformer.h.4.attn.bias\", \"transformer.h.5.attn.bias\". "
          ]
        }
      ],
      "source": [
        "# ==== ONE-CELL EXPORT PIPELINE FOR ANDROID (TORCHSCRIPT) ====\n",
        "# 1) Imports\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "import math, json\n",
        "\n",
        "# 2) Minimal modules (TorchScript-safe)\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "    def forward(self, x):\n",
        "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        # Always register mask as a Tensor (TorchScript-friendly)\n",
        "        self.register_buffer(\n",
        "            \"bias\",\n",
        "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
        "                1, 1, config.block_size, config.block_size\n",
        "            )\n",
        "        )\n",
        "        # Force non-flash path for JIT stability\n",
        "        self.flash = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        # Masked attention (scriptable)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        mask = self.bias[:, :, :T, :T]\n",
        "        att = att.masked_fill(mask == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# Simple dataclass only for rebuilding; we won't store it in the module.\n",
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int\n",
        "    vocab_size: int\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_embd: int\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        # Store primitives (TorchScript-safe), do NOT keep self.config\n",
        "        self.block_size = int(config.block_size)\n",
        "        self.vocab_size = int(config.vocab_size)\n",
        "        self.n_layer    = int(config.n_layer)\n",
        "        self.n_head     = int(config.n_head)\n",
        "        self.n_embd     = int(config.n_embd)\n",
        "        self.use_bias   = bool(config.bias)\n",
        "        self.drop_p     = float(config.dropout)\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(self.vocab_size, self.n_embd),\n",
        "            wpe=nn.Embedding(self.block_size, self.n_embd),\n",
        "            drop=nn.Dropout(self.drop_p),\n",
        "            h=nn.ModuleList([Block(config) for _ in range(self.n_layer)]),\n",
        "            ln_f=LayerNorm(self.n_embd, self.use_bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(self.n_embd, self.vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.n_layer))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "\n",
        "        # Truncate instead of assert\n",
        "        if t > self.block_size:\n",
        "            idx = idx[:, -self.block_size:]\n",
        "            t = idx.size(1)\n",
        "\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(\n",
        "                logits.reshape(-1, logits.size(-1)),\n",
        "                targets.reshape(-1),\n",
        "                ignore_index=-1\n",
        "            )\n",
        "            return logits, loss\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])  # [B,1,V]\n",
        "            return logits, None\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def generate(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "# 3) Rebuild from your saved config + load trained weights (NO retrain)\n",
        "cfg = GPTConfig(**json.load(open(\"config.json\")))\n",
        "model = GPT(cfg)\n",
        "state = torch.load(\"best_model_params.pt\", map_location=\"cpu\")\n",
        "model.load_state_dict(state, strict=True)\n",
        "model.eval()\n",
        "\n",
        "# 4) (Optional) shrink with dynamic INT8 quantization for Linear layers\n",
        "model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "# 5) Wrap so Android gets last-token logits [B, V]\n",
        "class ExportWrapper(nn.Module):\n",
        "    def __init__(self, m): super().__init__(); self.m = m\n",
        "    def forward(self, idx: torch.Tensor) -> torch.Tensor:  # idx: [B,T] int64\n",
        "        logits, _ = self.m(idx)\n",
        "        return logits[:, -1, :]  # [B, V]\n",
        "\n",
        "wrapper = ExportWrapper(model).eval()\n",
        "\n",
        "# 6) Export: try script, else fallback to trace (both OK for Android)\n",
        "try:\n",
        "    scripted = torch.jit.script(wrapper)\n",
        "    scripted.save(\"farmer_model_torchscript.pt\")\n",
        "    print(\"✅ Scripted TorchScript saved: farmer_model_torchscript.pt\")\n",
        "except Exception as e:\n",
        "    print(\"Scripting failed, using trace instead:\", e)\n",
        "    example = (torch.zeros(1, min(16, model.block_size), dtype=torch.long),)\n",
        "    traced = torch.jit.trace(wrapper, example)\n",
        "    traced.save(\"farmer_model_torchscript.pt\")\n",
        "    print(\"✅ Traced TorchScript saved: farmer_model_torchscript.pt\")\n",
        "\n",
        "# 7) Get tokenizer files (GPT-2 BPE) & download all Android assets\n",
        "try:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"huggingface_hub\"])\n",
        "    from huggingface_hub import hf_hub_download\n",
        "    import shutil\n",
        "    vocab_path  = hf_hub_download(repo_id=\"gpt2\", filename=\"vocab.json\")\n",
        "    merges_path = hf_hub_download(repo_id=\"gpt2\", filename=\"merges.txt\")\n",
        "    shutil.copy(vocab_path,  \"vocab.json\")\n",
        "    shutil.copy(merges_path, \"merges.txt\")\n",
        "    print(\"✅ Tokenizer files ready: vocab.json, merges.txt\")\n",
        "except Exception as e:\n",
        "    print(\"Tokenizer download skipped/failed:\", e)\n",
        "\n",
        "from google.colab import files\n",
        "for f in [\"farmer_model_torchscript.pt\", \"vocab.json\", \"merges.txt\"]:\n",
        "    try:\n",
        "        files.download(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Download {f} skipped:\", e)\n",
        "\n",
        "print(\"\\nDONE. Move these to android/app/src/main/assets/:\")\n",
        "print(\" - farmer_model_torchscript.pt\")\n",
        "print(\" - vocab.json\")\n",
        "print(\" - merges.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202,
          "referenced_widgets": [
            "4bd51e21ea6d4c8ca037c7362b5a7238",
            "1f297e9205124841be76eeaf741c60d1",
            "4fc01bb7061f4f928ca5db5f82dc9f92",
            "560d796d2e284f0bb19b83f7f507787f",
            "33fd359e571a49dcba5823ee1162aa95",
            "e84ab65ae996435597212537d284c5bf",
            "777e484a0112417197542abaebfb98db",
            "e37a1be4b33e4ac7a4becf79308350f5",
            "04ee7c03a5d7445fbfd69d8c980ede57",
            "285474f1204f4c859b1f99a4c8224f37",
            "b67c45b1179448e4bdd9930b1441e57c",
            "e4a12d67a18143988b2e7721c21dad35",
            "2558b71b422f4b9aa117dcd3b23820da",
            "628888be433a45e3bcbb637a22d0e68c",
            "53b6eac63cf240b6859dbd1621b9bf80",
            "350eaa9f2b8945ac9bdfb7174e0f765c",
            "7ec087c5aa6947f68c7ae6f77c8a8021",
            "593e5cb6d3f448e5b09e750b24a791f3",
            "d4f8919997804d2b9462eaf4d34eb5c3",
            "37335df05c19417799883c3896767811",
            "95ae9aa850c64d39ab2848ece96858ea",
            "811854a61f1c4e50aa9045d02463081a"
          ]
        },
        "id": "326BAKpemwc0",
        "outputId": "2a04951e-75d8-4f9f-b581-be0d99809b32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Traced TorchScript saved: farmer_model_torchscript.pt\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bd51e21ea6d4c8ca037c7362b5a7238",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4a12d67a18143988b2e7721c21dad35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Tokenizer files ready: vocab.json, merges.txt\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_406bcd1f-500b-4196-a189-e055f9b11552\", \"farmer_model_torchscript.pt\", 108075650)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_49bb1408-ad4d-4a01-83af-5241ac2fe6d8\", \"vocab.json\", 1042301)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_33bcdb42-2fe8-4a37-a98c-a7576c086c3c\", \"merges.txt\", 456318)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Move these into your Flutter Android project at android/app/src/main/assets/:\n",
            " - farmer_model_torchscript.pt\n",
            " - vocab.json\n",
            " - merges.txt\n"
          ]
        }
      ],
      "source": [
        "# ==== ANDROID EXPORT (CPU trace, no retrain, no CUDA) ====\n",
        "import json, torch, torch.nn as nn, torch.nn.functional as F, math\n",
        "\n",
        "# Force CPU for export (quantized kernels are CPU-only)\n",
        "torch.set_default_device(\"cpu\")\n",
        "\n",
        "# ---------------------------\n",
        "# TorchScript-safe model defs\n",
        "# ---------------------------\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "    def forward(self, x):\n",
        "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        # causal mask buffer (not saved in state_dict so old ckpts load)\n",
        "        self.register_buffer(\n",
        "            \"bias\",\n",
        "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
        "                1,1,config.block_size,config.block_size\n",
        "            ),\n",
        "            persistent=False\n",
        "        )\n",
        "        self.flash = False  # keep JIT happy (use masked path)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B,T,self.n_head,C//self.n_head).transpose(1,2)  # [B,H,T,Dh]\n",
        "        q = q.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
        "        v = v.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1,2).contiguous().view(B,T,C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4*config.n_embd, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4*config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.mlp  = MLP(config)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size:int; vocab_size:int; n_layer:int; n_head:int; n_embd:int\n",
        "    dropout:float=0.0; bias:bool=True\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        # store primitives (not the dataclass)\n",
        "        self.block_size = int(config.block_size)\n",
        "        self.vocab_size = int(config.vocab_size)\n",
        "        self.n_layer = int(config.n_layer)\n",
        "        self.n_head  = int(config.n_head)\n",
        "        self.n_embd  = int(config.n_embd)\n",
        "        self.use_bias = bool(config.bias)\n",
        "        self.drop_p   = float(config.dropout)\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(self.vocab_size, self.n_embd),\n",
        "            wpe = nn.Embedding(self.block_size, self.n_embd),\n",
        "            drop= nn.Dropout(self.drop_p),\n",
        "            h   = nn.ModuleList([Block(config) for _ in range(self.n_layer)]),\n",
        "            ln_f= LayerNorm(self.n_embd, self.use_bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(self.n_embd, self.vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        for pn,p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2*self.n_layer))\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        B, T = idx.size()\n",
        "        if T > self.block_size:\n",
        "            idx = idx[:, -self.block_size:]; T = idx.size(1)\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
        "        x = self.transformer.wte(idx) + self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(x)\n",
        "        for blk in self.transformer.h: x = blk(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)),\n",
        "                                   targets.reshape(-1), ignore_index=-1)\n",
        "            return logits, loss\n",
        "        return self.lm_head(x[:, [-1], :]), None  # [B,1,V]\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def generate(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "# ---------------------------\n",
        "# Load your saved config + weights (CPU), no quant on CUDA\n",
        "# ---------------------------\n",
        "cfg = GPTConfig(**json.load(open(\"config.json\")))\n",
        "model = GPT(cfg)\n",
        "state = torch.load(\"best_model_params.pt\", map_location=\"cpu\")\n",
        "# allow missing non-persistent buffers\n",
        "model.load_state_dict(state, strict=False)\n",
        "model.eval()\n",
        "model.cpu()\n",
        "\n",
        "# --- OPTIONAL SIZE REDUCTION ---\n",
        "# Use dynamic quantization on CPU (linear layers). Comment this out if you prefer FP32.\n",
        "model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
        "# --------------------------------\n",
        "\n",
        "# Wrapper returning last-token logits [B,V]\n",
        "class ExportWrapper(nn.Module):\n",
        "    def __init__(self, m): super().__init__(); self.m = m\n",
        "    def forward(self, idx: torch.Tensor) -> torch.Tensor:  # [B,T] int64 on CPU\n",
        "        logits, _ = self.m(idx)\n",
        "        return logits[:, -1, :]  # [B,V]\n",
        "\n",
        "wrapper = ExportWrapper(model).eval()\n",
        "\n",
        "# ---- Export with TRACE ONLY (skip script to avoid picky JIT issues) ----\n",
        "example = (torch.zeros(1, min(16, model.block_size), dtype=torch.long),)  # CPU tensor\n",
        "traced = torch.jit.trace(wrapper, example)\n",
        "traced.save(\"farmer_model_torchscript.pt\")\n",
        "print(\"✅ Traced TorchScript saved: farmer_model_torchscript.pt\")\n",
        "\n",
        "# Grab tokenizer files (GPT-2 BPE)\n",
        "try:\n",
        "    import sys, subprocess, shutil\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"huggingface_hub\"])\n",
        "    from huggingface_hub import hf_hub_download\n",
        "    shutil.copy(hf_hub_download(\"gpt2\", \"vocab.json\"), \"vocab.json\")\n",
        "    shutil.copy(hf_hub_download(\"gpt2\", \"merges.txt\"), \"merges.txt\")\n",
        "    print(\"✅ Tokenizer files ready: vocab.json, merges.txt\")\n",
        "except Exception as e:\n",
        "    print(\"Tokenizer download skipped/failed:\", e)\n",
        "\n",
        "# Download to your machine\n",
        "from google.colab import files\n",
        "for f in [\"farmer_model_torchscript.pt\", \"vocab.json\", \"merges.txt\"]:\n",
        "    try: files.download(f)\n",
        "    except Exception as e: print(f\"Download {f} skipped:\", e)\n",
        "\n",
        "print(\"\\nMove these into your Flutter Android project at android/app/src/main/assets/:\")\n",
        "print(\" - farmer_model_torchscript.pt\")\n",
        "print(\" - vocab.json\")\n",
        "print(\" - merges.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFLmO5qBnKet"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30732,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04ee7c03a5d7445fbfd69d8c980ede57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f015e2dafb14988b8bd74033e2a67f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41530173454b4cf3870d476ef7df20bd",
            "placeholder": "​",
            "style": "IPY_MODEL_5fb6f8f023b24e129000d9a7a4b7ffe7",
            "value": " 20000/20000 [3:21:03&lt;00:00,  2.16it/s]"
          }
        },
        "181545ca9a104f2381d2456c888d9a9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d12123b73fd74e6688c4bc39477373a4",
            "max": 20000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a04cc62546dd4c8694b657c576813987",
            "value": 20000
          }
        },
        "1f297e9205124841be76eeaf741c60d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e84ab65ae996435597212537d284c5bf",
            "placeholder": "​",
            "style": "IPY_MODEL_777e484a0112417197542abaebfb98db",
            "value": "vocab.json: 100%"
          }
        },
        "2558b71b422f4b9aa117dcd3b23820da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ec087c5aa6947f68c7ae6f77c8a8021",
            "placeholder": "​",
            "style": "IPY_MODEL_593e5cb6d3f448e5b09e750b24a791f3",
            "value": "merges.txt: 100%"
          }
        },
        "285474f1204f4c859b1f99a4c8224f37": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3128f40e72df4405b0c17046734d3d94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_634e38a972154967a3eba88b6fb68af9",
            "placeholder": "​",
            "style": "IPY_MODEL_47edcf5379fe48f996132064a496a663",
            "value": "100%"
          }
        },
        "33fd359e571a49dcba5823ee1162aa95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "350eaa9f2b8945ac9bdfb7174e0f765c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37335df05c19417799883c3896767811": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41530173454b4cf3870d476ef7df20bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47edcf5379fe48f996132064a496a663": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bd51e21ea6d4c8ca037c7362b5a7238": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f297e9205124841be76eeaf741c60d1",
              "IPY_MODEL_4fc01bb7061f4f928ca5db5f82dc9f92",
              "IPY_MODEL_560d796d2e284f0bb19b83f7f507787f"
            ],
            "layout": "IPY_MODEL_33fd359e571a49dcba5823ee1162aa95"
          }
        },
        "4fc01bb7061f4f928ca5db5f82dc9f92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e37a1be4b33e4ac7a4becf79308350f5",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04ee7c03a5d7445fbfd69d8c980ede57",
            "value": 1042301
          }
        },
        "53b6eac63cf240b6859dbd1621b9bf80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95ae9aa850c64d39ab2848ece96858ea",
            "placeholder": "​",
            "style": "IPY_MODEL_811854a61f1c4e50aa9045d02463081a",
            "value": " 456k/456k [00:00&lt;00:00, 7.04MB/s]"
          }
        },
        "560d796d2e284f0bb19b83f7f507787f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_285474f1204f4c859b1f99a4c8224f37",
            "placeholder": "​",
            "style": "IPY_MODEL_b67c45b1179448e4bdd9930b1441e57c",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 5.04MB/s]"
          }
        },
        "593e5cb6d3f448e5b09e750b24a791f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fb6f8f023b24e129000d9a7a4b7ffe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "628888be433a45e3bcbb637a22d0e68c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4f8919997804d2b9462eaf4d34eb5c3",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37335df05c19417799883c3896767811",
            "value": 456318
          }
        },
        "634e38a972154967a3eba88b6fb68af9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65ad3a86768b4ff1886bbe671dde8628": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "777e484a0112417197542abaebfb98db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ec087c5aa6947f68c7ae6f77c8a8021": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fc09e354ce24e9eaadc6378ce6953bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3128f40e72df4405b0c17046734d3d94",
              "IPY_MODEL_181545ca9a104f2381d2456c888d9a9c",
              "IPY_MODEL_0f015e2dafb14988b8bd74033e2a67f9"
            ],
            "layout": "IPY_MODEL_65ad3a86768b4ff1886bbe671dde8628"
          }
        },
        "811854a61f1c4e50aa9045d02463081a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95ae9aa850c64d39ab2848ece96858ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a04cc62546dd4c8694b657c576813987": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b67c45b1179448e4bdd9930b1441e57c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d12123b73fd74e6688c4bc39477373a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4f8919997804d2b9462eaf4d34eb5c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e37a1be4b33e4ac7a4becf79308350f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4a12d67a18143988b2e7721c21dad35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2558b71b422f4b9aa117dcd3b23820da",
              "IPY_MODEL_628888be433a45e3bcbb637a22d0e68c",
              "IPY_MODEL_53b6eac63cf240b6859dbd1621b9bf80"
            ],
            "layout": "IPY_MODEL_350eaa9f2b8945ac9bdfb7174e0f765c"
          }
        },
        "e84ab65ae996435597212537d284c5bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
